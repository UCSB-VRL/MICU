'''
Created on Jan 31, 2014
Versions:
    Feb1
    Feb2
    May9

Uses:
1) pyopenni to acquire: depth, rgb, & mask(user pixels) from primesense device
2) numpy to convert openni structures/data to mat arrays
3) opencv for display and video recording

Status: operational
    Only records/displays frames AFTER user is detected!!
    terminate code by pressing "Esc"

Current features:
    1) Saves png for types for the run: rgb, depth, mask, skel, all
        rgbdm is saved at 4:1
        NOTE: skelton & 'useful' mask appear when user is being tracked, else
            blank frames are saved.


    2) Displays the images: normal, medium, or small
        rgb, skeleton_joints, depth,mask
        Currently displaying small

    3) Extracts joints (beow is original pyopenni format)
        type(joint) -> class
        type(joint.point) -> list
        len(joint.point) -> 3
        type(joint.confidence)-> float

    4) Realworld and Projective joint coordinates stored in dictionaries
        pyopenni joints -> r_jnts={} and p_jnts={}
        keys:= str, ["head", "torso", "l_shoulder", ... r_shoulder, etc.]
        values: = list,[x,y,z,confidence]where elements are floats

    5) Create an HDF5 file (pytable)
        name: 'mmARC_device#.h5'
        group: 'action'
        table: 'actor'
            ||globalframe|viewframe|jnt_confidence|realworld[xyz]|...
             |projective[xyz]|timestamp|viewangle|actionlabel|actorlabel||

TODO:
    1) Create a dictionary w frame number, user id, and joint+confidence
    2) Save joints to respective frame
    3) Post-processes: save ALL info to hdf5
        action: str, name of the action E.g., kick
        angle: int, orientation angle wrt to front facing dev. E.g., [0:40:320]
        actor: str, actor name
        dev: str, device indx label # -> "dev1"
        projective: dictionary, projective 15-joint coordinates.
            key := joint; value:= [x,y,z,conf], list of floats
        realworld: dictionary, real world 15-joint coordinates [x y z]
        time: foat, time stamp from system/computer
        frame number : frame
        name: str, name used to save the rgb image (use: str.replace(rgb,mask))
        status: Bool, currently tracking a user (any user) o the scene

        dictionary: carmine={}
        keys=[tracking, confidence, projective, realWorld, timestamp, device,
              action, actor, angle, imname,userid]#, pose, sequence, bmom, bRT,
              gRT, gmom, hogRGB, hogDepth]

@author: Carlos
'''
#!/usr/bin/python

from openni import *
import numpy as np
import cv
import sys
import cv2
import time
import os

import tables as tb

import arctable as arc
from tables import *

XML_FILE = 'config.xml'
MAX_DEPTH_SIZE = 10000

context = Context()
context.init_from_xml_file(XML_FILE)

depth_generator = DepthGenerator()
depth_generator.create(context)

image_generator = ImageGenerator()
image_generator.create(context)

user_generator = UserGenerator()
user_generator.create(context)

user_generator.alternative_view_point_cap.set_view_point(image_generator)

palette = [(0, 0, 0), (255, 0, 0), (255, 0, 0), (255, 0, 0)]

grayscale_palette = tuple([(i, i, i) for i in range(256)])

# recoding parameters
dev         = 1 # device number
h5filename  = '../data/TESTmmARC_dev1.h5'
actorname   = 'carlos'
actionname  = 'testing'
view = 0

running = True
histogram = None
depth_map = None
image_count = 0
total_time = 0

# flags;
vis = True
save_frames = False
generate_videos = False

# drawing
radius = 5
green  = (0,255,0)
blue   = (255,0,0)
red    = (0,0,255)
colors = [green, blue, red]
confs  = [1.0, 0.5, 0.0]

x = 480/2
y = 640/2

# skeleton-joint handler:
handler  = {"head":         SKEL_HEAD,
            "neck":         SKEL_NECK,
            "torso":        SKEL_TORSO,
            "l_shoulder":   SKEL_LEFT_SHOULDER,
            "l_elbow":      SKEL_LEFT_ELBOW,
            "l_hand":       SKEL_LEFT_HAND,
            "l_hip":        SKEL_LEFT_HIP,
            "l_knee":       SKEL_LEFT_KNEE,
            "l_foot":       SKEL_LEFT_FOOT,
            "r_shoulder":   SKEL_RIGHT_SHOULDER,
            "r_elbow":      SKEL_RIGHT_ELBOW,
            "r_hand":       SKEL_RIGHT_HAND,
            "r_hip":        SKEL_RIGHT_HIP,
            "r_knee":       SKEL_RIGHT_KNEE,
            "r_foot":       SKEL_RIGHT_FOOT
          }
# handler


# array to store the image modalities+overlayed_skeleton (4images)
rgb   = np.zeros((480,640,3), np.uint8)
rgbdm = np.zeros((480,640*4, 3), np.uint8)

#check and/or generate the folder to store the images:
p = "../data/frames/"
if not os.path.isdir(p):
    print "creating folder"
    os.makedirs(p)

# Pose to use to calibrate the user
pose_to_use ='Psi'

# Obtain the skeleton & pose detection capabilities
skel_cap = user_generator.skeleton_cap
pose_cap = user_generator.pose_detection_cap

# Declare the callbacks
def new_user(src, id):
    print "1/4 User {} detected. Looking for pose..." .format(id)
    pose_cap.start_detection(pose_to_use, id)

def pose_detected(src, pose, id):
    print "2/4 Detected pose {} on user {}. Requesting calibration..." .format(pose,id)
    pose_cap.stop_detection(id)
    skel_cap.request_calibration(id, True)

def calibration_start(src, id):
    print "3/4 Calibration started for user {}." .format(id)

def calibration_complete(src, id, status):
    if status == CALIBRATION_STATUS_OK:
        print "4/4 User {} calibrated successfully! Starting to track." .format(id)
        skel_cap.start_tracking(id)
    else:
        print "ERR User {} failed to calibrate. Restarting process." .format(id)
        new_user(user_generator, id)

def lost_user(src, id):
    print "--- User {} lost." .format(id)

# Register them
user_generator.register_user_cb(new_user, lost_user)
pose_cap.register_pose_detected_cb(pose_detected)
skel_cap.register_c_start_cb(calibration_start)
skel_cap.register_c_complete_cb(calibration_complete)

# Set the profile
skel_cap.set_profile(SKEL_PROFILE_ALL)

# Start generating
context.start_generating_all()
print "0/4 Starting to detect users. Press Esc to exit."

##print "Image dimensions ({full_res[0]}, {full_res[1]})".format(full_res=depth_generator.metadata.full_res)

def calc_histogram():
    global histogram, depth_map
    max_depth = 0
    num_points = 0

    depth_map = np.asarray(depth_generator.get_tuple_depth_map())
    reduced_depth_map = depth_map[depth_map != 0]
    reduced_depth_map = reduced_depth_map[reduced_depth_map < MAX_DEPTH_SIZE]

    max_depth = min(reduced_depth_map.max(), MAX_DEPTH_SIZE)

    histogram = np.bincount(reduced_depth_map)
    num_points = len(reduced_depth_map)

    for i in xrange(1, max_depth): histogram[i] += histogram[i-1]

    if num_points > 0:
        histogram = 256 * (1.0-(histogram / float(num_points)))
# calc_histogram

def update_depth_image():
    calc_histogram()
    depth_frame = np.arange(640*480, dtype=np.uint32)
    depth_frame = histogram[depth_map[depth_frame]]
    depth_frame = depth_frame.reshape(480, 640) # float 64
##    depth_display = np.uint8(depth_frame/np.max(depth_frame) *255)
    #minMaxLoc( depth, &min, &max); //, Point &minLoc, Point &maxLoc );
    max, min = depth_frame.max(), depth_frame.min()
    depth_display=cv2.convertScaleAbs(depth_frame, 255/(max-min))
    return depth_frame, depth_display
# update_depth_image

def capture_rgb():
    '''Get rgb stream from primesense and convert it to an rgb numpy array'''
    bgr_frame = np.fromstring(image_generator.get_raw_image_map_bgr(), dtype=np.uint8).reshape(480, 640, 3)
    image = cv.fromarray(bgr_frame)
    cv.CvtColor(cv.fromarray(bgr_frame), image, cv.CV_BGR2RGB)
    # this generates a standard np array -- uncomment to test
    rgb = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)
    return rgb
# capture_rgb

def capture_mask():
    '''Get mask from pyopenni user_generator [0,1].
    mask:= numpy array, single channel in [0 255] range'''
    #mask:= binary [0,1], converted to [0,255]
    mask = np.uint8(np.asarray(user_generator.get_user_pixels(0)).reshape(480, 640)*255)
    return mask
#capture_mask

def save_frames(image_count, rgb,depth,mask,skel, rgbdm):#,depth,mask, n):
    '''Saves the images to as lossless pngs and appends the frame number n'''
    # save te images to the path
    cv2.imwrite(p+"rgb_"+str(image_count)+".png",rgb)
    cv2.imwrite(p+"depth_"+str(image_count)+".png",depth)
    cv2.imwrite(p+"mask_"+str(image_count)+".png",mask)
    cv2.imwrite(p+"skel_"+str(image_count)+".png",skel)
    cv2.imwrite(p+"all_"+str(image_count)+".png",rgbdm)

    return
# save_frames

def convert2projective(joint):
    '''Convert pyopenni joint_object into a list of floats:[x,y, z, confidence]
    x, y, z, and confidence are floats'''
    pt = depth_generator.to_projective([joint.point])[0]
    projective_joint= [float(pt[0]), float(pt[1]), float(pt[2])]#, joint.confidence]
    return projective_joint
#convert2projective

def get_joints(id):
    '''Extract/convert real-world joints to projective
    key:= , str joint label [head, neck, lshoulder, rshoulder]
    value:= float, [x,y,z, confidence]
    input:
        id:= int, user id number for which joint coorindates are needed
    outputs:
        p_joints:= dictionary, projective joints coordinates
        r_joints:= dictionary of real-world joint coordinates (same format)
            value:= list [float, float, float ,float]
            key:= str, joint label (see below)
    >>> get_joints(int)-> dictionary, dictionary
    Accessing dictionary:
        >>> p[head] ->  [20.0, 30.0, 10.0, 0.5]
    '''
    # initialize the dictionaries:
    r={}
    p={}
    real_w = {}
    for key in handler.keys():
        r[key] = skel_cap.get_joint_position(id,handler[key])# -> [str,str,str,float]
        # Convert to projective
        p[key] = convert2projective(r[key])
        # Convert the data in the original dictonary to format
        real_w[key] = [ float(r[key].point[0]),float(r[key].point[1]),
                        float(r[key].point[2]),r[key].confidence]
        # confidences:
    return p, real_w
# get_joints


def get_joint_arrays(id):
    '''Extract/convert real-world joints to projective
    key:= , str joint label [head, neck, lshoulder, rshoulder]
    value:= float, [x,y,z, confidence]
    input:
        id:= int, user id number for which joint coorindates are needed
    outputs:
        p_joints:= dictionary, projective joints coordinates
        r_joints:= dictionary of real-world joint coordinates (same format)
            value:= list [float, float, float ,float]
            key:= str, joint label (see below)
    >>> get_joints(int)-> dictionary, dictionary
    Accessing dictionary:
        >>> p[head] ->  [20.0, 30.0, 10.0, 0.5]
    '''
    # initialize the dictionaries:
    r={}
    p={}
    real_w = {}
    real_list = []
    proj_list = []
    conf_list =[]
    for key in handler.keys():
        r[key] = skel_cap.get_joint_position(id,handler[key])# -> [str,str,str,float]
        # Convert to projective
        p[key] = convert2projective(r[key])
        # Convert the data in the original dictonary to format
        real_w[key] = [ float(r[key].point[0]),float(r[key].point[1]),
                        float(r[key].point[2])]#,r[key].confidence]
        #convert to list
        conf_list.append(r[key].confidence)
        proj_list.append(p[key])
        real_list.append(real_w[key])
    # convert to array
    confidences = np.array(proj_list)#.reshape(15,1)
    proj_coords = (np.array(proj_list)).reshape(15,3)
    real_coords = (np.array(real_list)).reshape(15,3)

    print confidences.shape, type(confidences)
        # confidences:
    return real_coords, real_coords, confidences
# get_joint_arrays






## ===========================================================================
# Functions for the hdf5 file management
## ---------------------------------------------------------------------------
# check if h5 file exists
def checkh5exists(filename):
    '''Check if hdf5 file exists
    if not: create it and open is to add new groups and tables
    if yes: open it to append new data --> new group and/or table
    input:
        filename: str'''
    if not os.path.isfile(filename):
        print "creating hdf5 file: ", filename
        h5file= openFile(filename, mode="w", title = "TESTmmARC_device1")
        group = h5file.createGroup("/", actionname,"actions")
    else:
        print "open hdf5 to append: ", filename
        h5file= openFile(filename, mode="a", title = "TESTmmARC_device1")
        group = "/"+actionname

    return h5file, group
#checkh5exists



## ======== MAIN LOOP =========
# Verify that the hdf5 file exists
h5file, group  = checkh5exists(h5filename)

#create a new table: devTable
devTable = h5file.createTable(group, actorname, arc.ARCtable, actorname)


# initialize the arrays
confidences = np.zeros((15,1), dtype=float)
p_jnts = np.zeros((15,3), dtype=float)
r_jnts = np.zeros((15,3), dtype=float)


while running:
    key = cv2.waitKey(1)
    if (key == 27):# or total_time >= minutes*60):
        print "Terminating code!"
        print "closing hdf5 file"
        h5file.close()
        running = False
# populate the table one row at a time: devRow
    devRow = devTable.row

    context.wait_any_update_all()
    tic = time.time()
# collect images from carmine - even w/o a user detected
    rgb   = capture_rgb()
    depth,depth4display = update_depth_image()
    skel  = np.ones((480,640, 3), np.uint8)*255
    cv2.putText(skel,"NO USER",(x,y), cv2.FONT_HERSHEY_PLAIN, 2.0, red,
    thickness=2, lineType=cv2.CV_AA)
    mask  = capture_mask()

    # Extract head position of each tracked user
    for id in user_generator.users: # Consider only one user by ussing [0]
        if skel_cap.is_tracking(id):
            # Get the frames
            rgb   = capture_rgb()
            depth,depth4display = update_depth_image()
            mask  = capture_mask()
            skel  = rgb.copy()

##            p_jnts, r_jnts, confidence = get_joints(id) # projective and real coordnates
##            #draw joints:
##            for key in p_jnts.keys():
##                center = (int(p_jnts[key][0]), int(p_jnts[key][1]))
##                conf = p_jnts[key][3]
##                color = colors[confs.index(conf)]
            p_jnts, r_jnts, confidences = get_joints_arrays(id) # projective and real coordnates
            #draw joints:
            for i in arange(14):
                center = (int(p_jnts[i,0]), int(p_jnts[i,1]))
                conf = confidence[i]
                color = colors[confs.index(conf)]
                cv2.circle(skel, center ,radius, color, thickness=-2)
        #if skel_cap
    #for id

    devRow['globalframe'] = image_count
    devRow['viewframe']   = image_count
    #devRow['confidence']  = confidences
    devRow['realworld']   = r_jnts
    devRow['projective']  = p_jnts
    devRow['timestamp']   = time.time()
    devRow['viewangle']   = view
    devRow['actionlabel'] = 0
    devRow['actorlabel']  = 1
    devRow['actionname']  = actionname
    devRow['actorname']   = actorname

    devRow.append()

    # check the flags
    if (vis or save_frames):
        rgbdm[:,0:640]    = rgb
        rgbdm[:,640:1280] = skel
        rgbdm[:,1280:1920]= cv2.cvtColor(depth4display,cv2.COLOR_GRAY2RGB)
        rgbdm[:,1920:2560]= cv2.cvtColor(mask,cv2.COLOR_GRAY2RGB)
        #rgbdm_small = rgbdm # orginal size
        #rgbdm_small = cv2.resize(rgbdm,(1280,240))# medium
        rgbdm_small = cv2.resize(rgbdm,(640,120)) # smallest
        if vis:
            # display the concatenated images
            cv2.imshow("4:1 scale", rgbdm_small) # small
        if save_frames:
            # Save the frames as png's
            save_frames(image_count,rgb,depth, mask, skel, rgbdm_small)
    #if vis or save_frames

    image_count += 1
    toc = time.time()
    total_time += toc-tic
    #print ("continuous fps: %.2f" % image_count/total_time)
# while


fps = image_count/total_time

# print some info:
print ("total run time is %.2f secs" % total_time)
print ("fps: %.2f" %fps)

context.stop_generating_all()

# generate the videos
if (save_frames and generate_videos):
    os.system ("python carmine_generate_vids.py ")

sys.exit(0)